//! Cross-crate integration tests for the Cortex cognitive memory system
//!
//! This test suite validates that all cortex crates work together correctly,
//! testing:
//! 1. cortex-core + cortex-storage: Configuration â†’ Database setup
//! 2. cortex-storage + cortex-vfs: VFS uses connection pool
//! 3. cortex-vfs + cortex-memory: Memory stores VFS metadata
//! 4. cortex-memory + cortex-semantic: Episodes embedded and searchable
//! 5. cortex-ingestion + cortex-semantic: Documents chunked and embedded
//! 6. cortex-mcp + ALL: Tools integrate with all systems
//! 7. cortex-cli + ALL: CLI commands work correctly
//!
//! Each test verifies proper data flow, error handling, and API compatibility
//! between crates.

use cortex_core::prelude::*;
use cortex_storage::prelude::*;
use cortex_storage::connection_pool::ConnectionMode;
use cortex_vfs::prelude::*;
use cortex_memory::prelude::*;
// Explicitly use cortex_memory::types::CodeUnitType for SemanticUnit
use cortex_memory::types::CodeUnitType;
use cortex_semantic::prelude::*;
use cortex_ingestion::prelude::*;
use cortex_ingestion::ChunkType;

use std::path::PathBuf;
use std::sync::Arc;
use std::time::Duration;
use tempfile::TempDir;
use tokio::fs;

// =============================================================================
// Helper Functions
// =============================================================================

/// Create a test database configuration from GlobalConfig
fn create_test_db_config_from_global(global_config: &GlobalConfig) -> DatabaseConfig {
    let db_config = global_config.database();
    DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials {
            username: Some(db_config.username.clone()),
            password: Some(db_config.password.clone()),
        },
        pool_config: cortex_storage::prelude::PoolConfig {
            min_connections: global_config.pool().min_connections as usize,
            max_connections: global_config.pool().max_connections as usize,
            connection_timeout: Duration::from_millis(global_config.pool().connection_timeout_ms),
            idle_timeout: Some(Duration::from_millis(global_config.pool().idle_timeout_ms)),
            ..Default::default()
        },
        namespace: "cortex_integration_test".to_string(),
        database: "test".to_string(),
    }
}

/// Create a test workspace
async fn create_test_workspace() -> (TempDir, PathBuf) {
    let temp_dir = TempDir::new().unwrap();
    let workspace_path = temp_dir.path().join("test_workspace");
    fs::create_dir_all(&workspace_path).await.unwrap();
    (temp_dir, workspace_path)
}

/// Create test files in a workspace
async fn create_test_files(workspace: &PathBuf) -> Vec<PathBuf> {
    let mut files = Vec::new();

    // Rust source file
    let rust_file = workspace.join("lib.rs");
    fs::write(
        &rust_file,
        r#"
pub fn add(a: i32, b: i32) -> i32 {
    a + b
}

pub fn multiply(a: i32, b: i32) -> i32 {
    a * b
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_add() {
        assert_eq!(add(2, 2), 4);
    }

    #[test]
    fn test_multiply() {
        assert_eq!(multiply(3, 4), 12);
    }
}
"#,
    )
    .await
    .unwrap();
    files.push(rust_file);

    // Markdown documentation
    let readme = workspace.join("README.md");
    fs::write(
        &readme,
        r#"# Integration Test Project

This project tests cross-crate integration in Cortex.

## Features

- Mathematical operations (add, multiply)
- Comprehensive test coverage
- Documentation

## Usage

```rust
use integration_test::{add, multiply};

let sum = add(5, 3);
let product = multiply(4, 7);
```
"#,
    )
    .await
    .unwrap();
    files.push(readme);

    // Configuration file
    let config = workspace.join("Cargo.toml");
    fs::write(
        &config,
        r#"[package]
name = "integration-test"
version = "0.1.0"
edition = "2021"

[dependencies]
# Add dependencies here
"#,
    )
    .await
    .unwrap();
    files.push(config);

    files
}

// =============================================================================
// Test 1: cortex-core + cortex-storage Integration
// =============================================================================

#[tokio::test]
async fn test_core_storage_config_to_database() {
    // Create GlobalConfig with test profile
    let global_config = GlobalConfig::with_profile(ConfigProfile::Test);

    // Verify configuration values
    assert_eq!(global_config.profile(), ConfigProfile::Test);
    assert_eq!(global_config.general().log_level, "warn");
    assert_eq!(global_config.database().namespace, "cortex_test");

    // Convert GlobalConfig to DatabaseConfig
    let db_config = create_test_db_config_from_global(&global_config);

    // Verify DatabaseConfig created correctly from GlobalConfig
    assert_eq!(db_config.namespace, "cortex_integration_test");
    assert_eq!(db_config.database, "test");
    assert_eq!(db_config.pool_config.min_connections, 2);
    assert_eq!(db_config.pool_config.max_connections, 2);

    // Create ConnectionManager with converted config
    let connection_manager = ConnectionManager::new(db_config)
        .await
        .expect("Failed to create connection manager");

    // Verify connection manager is operational
    let health = connection_manager.health_status();
    assert!(health.healthy);
}

#[tokio::test]
async fn test_core_storage_error_types_compatibility() {
    // Test that CortexError can represent storage errors
    let cortex_error = CortexError::Storage("Database connection failed".to_string());
    assert!(matches!(cortex_error, CortexError::Storage(_)));

    // Test error message extraction
    let error_msg = format!("{}", cortex_error);
    assert!(error_msg.contains("Database connection failed"));
}

#[tokio::test]
async fn test_core_storage_id_types_in_queries() {
    // Create a CortexId
    let id = CortexId::new();

    // Verify ID can be serialized (for use in database queries)
    let id_str = id.to_string();
    assert!(!id_str.is_empty());

    // Verify ID is consistent
    let id_str2 = id.to_string();
    assert_eq!(id_str, id_str2);
}

#[tokio::test]
async fn test_core_storage_metadata_serialization() {
    use serde_json;

    // Create GlobalConfig
    let config = GlobalConfig::default();
    let metadata = config.metadata();

    // Serialize to JSON
    let json = serde_json::to_string(&metadata).expect("Failed to serialize metadata");
    assert!(json.contains("version"));
    assert!(json.contains("profile"));

    // Deserialize back
    let deserialized: ConfigMetadata =
        serde_json::from_str(&json).expect("Failed to deserialize metadata");
    assert_eq!(deserialized.version, metadata.version);
    assert_eq!(deserialized.profile, metadata.profile);
}

// =============================================================================
// Test 2: cortex-storage + cortex-vfs Integration
// =============================================================================

#[tokio::test]
async fn test_storage_vfs_connection_pool_usage() {
    // Create database config
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_test".to_string(),
        database: "vfs".to_string(),
    };

    // Create connection manager
    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    // Create VFS with connection pool
    let vfs = VirtualFileSystem::new(connection_manager.clone());

    // Create workspace
    let workspace_id = uuid::Uuid::new_v4();
    let now = chrono::Utc::now();
    let workspace = Workspace {
        id: workspace_id,
        name: "test-workspace".to_string(),
        workspace_type: WorkspaceType::Code,
        source_type: SourceType::Local,
        namespace: "cortex_integration_test".to_string(),
        source_path: None,
        read_only: false,
        parent_workspace: None,
        fork_metadata: None,
        created_at: now,
        updated_at: now,
    };

    // Write a file to VFS (should use connection pool internally)
    let path = VirtualPath::new("test.txt").expect("Invalid path");
    vfs.write_file(&workspace_id, &path, b"Hello, VFS!")
        .await
        .expect("Failed to write file");

    // Read file back (should use connection pool)
    let content = vfs
        .read_file(&workspace_id, &path)
        .await
        .expect("Failed to read file");

    assert_eq!(content, b"Hello, VFS!");

    // Verify connection pool health
    let health = connection_manager.health_status();
    assert!(health.healthy);
}

#[tokio::test]
async fn test_storage_vfs_transaction_support() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_tx_test".to_string(),
        database: "vfs_tx".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = VirtualFileSystem::new(connection_manager.clone());
    let workspace_id = uuid::Uuid::new_v4();

    // Create multiple files in sequence (simulating transactional writes)
    let files = vec!["file1.txt", "file2.txt", "file3.txt"];

    for file_name in &files {
        let path = VirtualPath::new(file_name).expect("Invalid path");
        vfs.write_file(&workspace_id, &path, file_name.as_bytes())
            .await
            .expect("Failed to write file");
    }

    // Verify all files exist
    for file_name in &files {
        let path = VirtualPath::new(file_name).expect("Invalid path");
        let exists = vfs.exists(&workspace_id, &path).await.unwrap_or(false);
        assert!(exists, "File {} should exist", file_name);
    }
}

#[tokio::test]
async fn test_storage_vfs_batch_operations() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_batch_test".to_string(),
        database: "vfs_batch".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = VirtualFileSystem::new(connection_manager);
    let workspace_id = uuid::Uuid::new_v4();

    // Batch write operation: create directory structure
    let base_path = VirtualPath::new("src").expect("Invalid path");
    vfs.create_directory(&workspace_id, &base_path, true)
        .await
        .expect("Failed to create directory");

    // Write multiple files in the directory
    let files = vec!["main.rs", "lib.rs", "utils.rs"];
    for file_name in &files {
        let file_path = base_path
            .join(file_name)
            .expect("Failed to join path");
        vfs.write_file(&workspace_id, &file_path, b"// Rust code")
            .await
            .expect("Failed to write file");
    }

    // List directory contents (batch read)
    let entries = vfs
        .list_directory(&workspace_id, &base_path, false)
        .await
        .expect("Failed to list directory");

    assert_eq!(entries.len(), files.len());
}

// =============================================================================
// Test 3: cortex-vfs + cortex-memory Integration
// =============================================================================

#[tokio::test]
async fn test_vfs_memory_file_metadata_storage() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_memory_test".to_string(),
        database: "vfs_memory".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = VirtualFileSystem::new(connection_manager.clone());
    let cognitive_manager = CognitiveManager::new(connection_manager);

    let workspace_id = uuid::Uuid::new_v4();
    let project_id = CortexId::new();

    // Create files in VFS
    let file_path = VirtualPath::new("important.rs").expect("Invalid path");
    vfs.write_file(&workspace_id, &file_path, b"fn important() {}")
        .await
        .expect("Failed to write file");

    // Store episode in memory about the file creation
    let mut episode = EpisodicMemory::new(
        "Created important.rs".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Feature,
    );

    episode.entities_created = vec!["important.rs".to_string()];
    episode.outcome = EpisodeOutcome::Success;

    let episode_id = cognitive_manager
        .remember_episode(&episode)
        .await
        .expect("Failed to store episode");

    // Retrieve episode and verify metadata
    let retrieved = cognitive_manager
        .episodic()
        .get_episode(episode_id)
        .await
        .expect("Failed to retrieve episode")
        .expect("Episode not found");

    assert_eq!(retrieved.entities_created.len(), 1);
    assert_eq!(retrieved.entities_created[0], "important.rs");
}

#[tokio::test]
async fn test_vfs_memory_content_hashing_integration() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_hash_test".to_string(),
        database: "vfs_hash".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = VirtualFileSystem::new(connection_manager.clone());
    let cognitive_manager = CognitiveManager::new(connection_manager);

    let workspace_id = uuid::Uuid::new_v4();
    let project_id = CortexId::new();

    // Write file with specific content
    let content = b"const VERSION: &str = \"1.0.0\";";
    let file_path = VirtualPath::new("version.rs").expect("Invalid path");
    vfs.write_file(&workspace_id, &file_path, content)
        .await
        .expect("Failed to write file");

    // Read file back
    let file_content = vfs
        .read_file(&workspace_id, &file_path)
        .await
        .expect("Failed to read file");

    // Verify content is correct
    assert_eq!(file_content, content);

    // Write same content again
    vfs.write_file(&workspace_id, &file_path, content)
        .await
        .expect("Failed to write file");

    let file_content2 = vfs
        .read_file(&workspace_id, &file_path)
        .await
        .expect("Failed to read file");

    // Content should still be the same (verifying VFS storage)
    assert_eq!(file_content, file_content2);

    // Store semantic unit with content hash
    let unit = SemanticUnit {
        id: CortexId::new(),
        unit_type: CodeUnitType::Const,
        name: "VERSION".to_string(),
        qualified_name: "version::VERSION".to_string(),
        display_name: "VERSION".to_string(),
        file_path: "version.rs".to_string(),
        start_line: 1,
        start_column: 0,
        end_line: 1,
        end_column: 31,
        signature: "const VERSION: &str".to_string(),
        body: String::from_utf8_lossy(content).to_string(),
        docstring: None,
        visibility: "public".to_string(),
        modifiers: vec!["const".to_string()],
        parameters: vec![],
        return_type: Some("&str".to_string()),
        summary: "Version constant".to_string(),
        purpose: "Store application version".to_string(),
        complexity: ComplexityMetrics {
            cyclomatic: 1,
            cognitive: 1,
            nesting: 0,
            lines: 1,
        },
        test_coverage: None,
        has_tests: false,
        has_documentation: false,
        embedding: None,
        created_at: chrono::Utc::now(),
        updated_at: chrono::Utc::now(),
    };

    cognitive_manager
        .remember_unit(&unit)
        .await
        .expect("Failed to store semantic unit");
}

#[tokio::test]
async fn test_vfs_memory_version_history() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_vfs_version_test".to_string(),
        database: "vfs_version".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = VirtualFileSystem::new(connection_manager.clone());
    let cognitive_manager = CognitiveManager::new(connection_manager);

    let workspace_id = uuid::Uuid::new_v4();
    let project_id = CortexId::new();
    let file_path = VirtualPath::new("evolving.rs").expect("Invalid path");

    // Version 1
    vfs.write_file(&workspace_id, &file_path, b"fn version_1() {}")
        .await
        .expect("Failed to write file");

    let mut episode1 = EpisodicMemory::new(
        "Created version 1".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Feature,
    );
    episode1.entities_created = vec!["evolving.rs".to_string()];
    cognitive_manager
        .remember_episode(&episode1)
        .await
        .expect("Failed to store episode");

    // Version 2
    vfs.write_file(&workspace_id, &file_path, b"fn version_2() {}")
        .await
        .expect("Failed to write file");

    let mut episode2 = EpisodicMemory::new(
        "Updated to version 2".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Refactor,
    );
    episode2.entities_modified = vec!["evolving.rs".to_string()];
    cognitive_manager
        .remember_episode(&episode2)
        .await
        .expect("Failed to store episode");

    // Query episodes for this project
    let episodes = cognitive_manager
        .episodic()
        .get_episodes_for_project(project_id)
        .await
        .expect("Failed to get episodes");

    assert_eq!(episodes.len(), 2);
    assert_eq!(episodes[0].task_description, "Created version 1");
    assert_eq!(episodes[1].task_description, "Updated to version 2");
}

// =============================================================================
// Test 4: cortex-memory + cortex-semantic Integration
// =============================================================================

#[tokio::test]
async fn test_memory_semantic_episodes_and_patterns() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_memory_semantic_test".to_string(),
        database: "memory_semantic".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let cognitive_manager = CognitiveManager::new(connection_manager);
    let project_id = CortexId::new();

    // Create semantic units
    let unit = SemanticUnit {
        id: CortexId::new(),
        unit_type: CodeUnitType::Function,
        name: "process_data".to_string(),
        qualified_name: "module::process_data".to_string(),
        display_name: "process_data".to_string(),
        file_path: "processor.rs".to_string(),
        start_line: 10,
        start_column: 0,
        end_line: 30,
        end_column: 1,
        signature: "pub fn process_data(input: &[u8]) -> Result<Vec<u8>>".to_string(),
        body: "// Complex processing logic".to_string(),
        docstring: Some("Process input data".to_string()),
        visibility: "public".to_string(),
        modifiers: vec![],
        parameters: vec!["input".to_string()],
        return_type: Some("Result<Vec<u8>>".to_string()),
        summary: "Data processing function".to_string(),
        purpose: "Transform input data".to_string(),
        complexity: ComplexityMetrics {
            cyclomatic: 10,
            cognitive: 15,
            nesting: 4,
            lines: 20,
        },
        test_coverage: Some(0.8),
        has_tests: true,
        has_documentation: true,
        embedding: None,
        created_at: chrono::Utc::now(),
        updated_at: chrono::Utc::now(),
    };

    let unit_id = cognitive_manager
        .remember_unit(&unit)
        .await
        .expect("Failed to store semantic unit");

    // Create episode referencing the semantic unit
    let mut episode = EpisodicMemory::new(
        "Optimized process_data function".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Feature,
    );

    episode.entities_modified = vec!["processor.rs".to_string()];
    episode.outcome = EpisodeOutcome::Success;

    cognitive_manager
        .remember_episode(&episode)
        .await
        .expect("Failed to store episode");

    // Create learned pattern
    let pattern = LearnedPattern::new(
        PatternType::Optimization,
        "Cache intermediate results".to_string(),
        "Store computed values to avoid recomputation".to_string(),
        "Performance optimization".to_string(),
    );

    let pattern_id = cognitive_manager
        .remember_pattern(&pattern)
        .await
        .expect("Failed to store pattern");

    // Record pattern application
    cognitive_manager
        .procedural()
        .record_success(pattern_id)
        .await
        .expect("Failed to record success");

    // Verify cross-memory queries work
    let stats = cognitive_manager
        .get_statistics()
        .await
        .expect("Failed to get statistics");

    assert_eq!(stats.episodic.total_episodes, 1);
    assert_eq!(stats.semantic.total_units, 1);
    assert_eq!(stats.procedural.total_patterns, 1);
}

#[tokio::test]
async fn test_memory_semantic_consolidation_workflow() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_consolidation_test".to_string(),
        database: "consolidation".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let cognitive_manager = CognitiveManager::new(connection_manager);
    let project_id = CortexId::new();

    // Create multiple episodes with patterns
    for i in 0..5 {
        let mut episode = EpisodicMemory::new(
            format!("Task {}", i),
            "test-agent".to_string(),
            project_id,
            EpisodeType::Feature,
        );

        episode.tools_used = vec![ToolUsage {
            tool_name: "optimizer".to_string(),
            usage_count: 1,
            total_duration_ms: 1000,
            parameters: std::collections::HashMap::new(),
        }];

        episode.outcome = EpisodeOutcome::Success;

        cognitive_manager
            .remember_episode(&episode)
            .await
            .expect("Failed to store episode");
    }

    // Consolidate memories
    let report = cognitive_manager
        .consolidate()
        .await
        .expect("Failed to consolidate");

    assert!(report.duration_ms > 0);
    assert!(report.episodes_processed >= 0);

    // Dream to extract patterns
    let patterns = cognitive_manager
        .dream()
        .await
        .expect("Failed to dream");

    // Patterns may or may not be extracted depending on implementation
    assert!(patterns.len() >= 0);
}

// =============================================================================
// Test 5: cortex-ingestion + cortex-semantic Integration
// =============================================================================

#[tokio::test]
async fn test_ingestion_semantic_document_chunking() {
    // Create a test document
    let document = r#"
# Machine Learning Guide

Machine learning is a subset of artificial intelligence that enables systems to learn from data.

## Supervised Learning

Supervised learning uses labeled training data to learn patterns.

## Unsupervised Learning

Unsupervised learning finds patterns in unlabeled data.

## Reinforcement Learning

Reinforcement learning learns through trial and error with rewards and penalties.
"#;

    // Chunk the document
    let chunker = SemanticChunker::new(500, 50);
    let chunks = chunker.chunk(document);

    assert!(!chunks.is_empty());

    // Verify chunks are not empty
    for chunk in &chunks {
        assert!(!chunk.is_empty());
    }
}

#[tokio::test]
async fn test_ingestion_semantic_metadata_extraction() {
    use cortex_ingestion::extractor::*;

    let document = r#"
# Technical Documentation

This document describes the system architecture.

**Keywords**: architecture, system design, scalability

## Components

- Database layer
- API layer
- Frontend
"#;

    let metadata = extract_comprehensive_metadata(
        std::path::Path::new("doc.md"),
        document,
    );

    assert!(metadata.contains_key("word_count"));
    assert!(metadata.contains_key("language"));

    // Detect language
    let lang = detect_language(document);
    assert!(lang.is_some());
}

#[tokio::test]
async fn test_ingestion_semantic_quality_scoring() {
    let high_quality_doc = r#"
# Well-Documented Function

This function implements the Fibonacci sequence algorithm.

## Parameters

- `n`: The position in the Fibonacci sequence (0-indexed)

## Returns

The Fibonacci number at position `n`.

## Example

```rust
assert_eq!(fibonacci(10), 55);
```

## Complexity

- Time: O(n)
- Space: O(1)
"#;

    let low_quality_doc = "some random text";

    // Process high quality document
    let processor_factory = ProcessorFactory::new();
    let processor = processor_factory
        .get_for_path(std::path::Path::new("test.md"))
        .expect("Failed to get markdown processor");

    let high_quality = processor
        .process(high_quality_doc.as_bytes())
        .await
        .expect("Failed to process high quality doc");

    let low_quality = processor
        .process(low_quality_doc.as_bytes())
        .await
        .expect("Failed to process low quality doc");

    // High quality should have more chunks and metadata
    assert!(high_quality.chunks.len() > low_quality.chunks.len());
    assert!(high_quality.metadata.len() >= low_quality.metadata.len());
}

// =============================================================================
// Test 6: End-to-End Workflow Scenarios
// =============================================================================

#[tokio::test]
async fn test_e2e_create_workspace_ingest_search_modify_consolidate() {
    // Step 1: Create workspace
    let (_temp_dir, workspace_path) = create_test_workspace().await;
    let files = create_test_files(&workspace_path).await;

    // Step 2: Initialize database
    let global_config = GlobalConfig::with_profile(ConfigProfile::Test);
    let db_config = create_test_db_config_from_global(&global_config);

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    // Step 3: Initialize all systems
    let vfs = VirtualFileSystem::new(connection_manager.clone());
    let cognitive_manager = CognitiveManager::new(connection_manager.clone());

    let workspace_id = uuid::Uuid::new_v4();
    let project_id = CortexId::new();

    // Step 4: Ingest files into VFS
    for file_path in &files {
        let content = fs::read(file_path).await.expect("Failed to read file");
        let file_name = file_path.file_name().unwrap().to_string_lossy();
        let vpath = VirtualPath::new(&file_name).expect("Invalid path");

        vfs.write_file(&workspace_id, &vpath, &content)
            .await
            .expect("Failed to write file to VFS");
    }

    // Step 5: Create episode for ingestion
    let mut episode = EpisodicMemory::new(
        "Ingested project files".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Task,
    );

    episode.entities_created = files
        .iter()
        .map(|f| f.file_name().unwrap().to_string_lossy().to_string())
        .collect();
    episode.outcome = EpisodeOutcome::Success;

    cognitive_manager
        .remember_episode(&episode)
        .await
        .expect("Failed to store episode");

    // Step 6: Modify a file
    let lib_path = VirtualPath::new("lib.rs").expect("Invalid path");
    let new_content = b"pub fn add(a: i32, b: i32) -> i32 { a + b }";
    vfs.write_file(&workspace_id, &lib_path, new_content)
        .await
        .expect("Failed to modify file");

    // Step 7: Record modification
    let mut mod_episode = EpisodicMemory::new(
        "Simplified lib.rs".to_string(),
        "test-agent".to_string(),
        project_id,
        EpisodeType::Refactor,
    );
    mod_episode.entities_modified = vec!["lib.rs".to_string()];
    cognitive_manager
        .remember_episode(&mod_episode)
        .await
        .expect("Failed to store modification episode");

    // Step 8: Consolidate memories
    let report = cognitive_manager
        .consolidate()
        .await
        .expect("Failed to consolidate");

    assert!(report.duration_ms > 0);

    // Step 9: Verify final state
    let stats = cognitive_manager
        .get_statistics()
        .await
        .expect("Failed to get statistics");

    assert_eq!(stats.episodic.total_episodes, 2);

    // Verify VFS state
    let content = vfs
        .read_file(&workspace_id, &lib_path)
        .await
        .expect("Failed to read modified file");

    assert_eq!(content, new_content);
}

#[tokio::test]
async fn test_e2e_import_document_chunk_search_retrieve() {
    // Step 1: Create test document
    let document = r#"
# Rust Programming Language

Rust is a systems programming language focused on safety, speed, and concurrency.

## Memory Safety

Rust's ownership system ensures memory safety without garbage collection.

## Concurrency

Rust provides fearless concurrency through its type system.

## Performance

Rust has zero-cost abstractions and compiles to native code.
"#;

    // Step 2: Process document
    let processor_factory = ProcessorFactory::new();
    let processor = processor_factory
        .get_for_path(std::path::Path::new("rust_guide.md"))
        .expect("Failed to get markdown processor");

    let processed = processor
        .process(document.as_bytes())
        .await
        .expect("Failed to process document");

    // Step 3: Verify chunks
    assert!(!processed.chunks.is_empty());
    assert!(processed.chunks.len() >= 3); // At least 3 sections

    // Step 4: Extract metadata
    assert!(processed.metadata.contains_key("word_count"));

    // Step 5: Initialize database for search
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_e2e_doc_test".to_string(),
        database: "doc".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let cognitive_manager = CognitiveManager::new(connection_manager);

    // Step 6: Store chunks as semantic units
    for (idx, chunk) in processed.chunks.iter().enumerate() {
        let unit = SemanticUnit {
            id: CortexId::new(),
            unit_type: CodeUnitType::Module,
            name: format!("chunk_{}", idx),
            qualified_name: format!("rust_guide::chunk_{}", idx),
            display_name: format!("Chunk {}", idx),
            file_path: "rust_guide.md".to_string(),
            start_line: idx as u32,
            start_column: 0,
            end_line: (idx + 1) as u32,
            end_column: 0,
            signature: String::new(),
            body: chunk.content.clone(),
            docstring: Some(chunk.content.clone()),
            visibility: "public".to_string(),
            modifiers: vec![],
            parameters: vec![],
            return_type: None,
            summary: format!("Documentation chunk {}", idx),
            purpose: "Store documentation".to_string(),
            complexity: ComplexityMetrics {
                cyclomatic: 1,
                cognitive: 1,
                nesting: 0,
                lines: chunk.content.lines().count() as u32,
            },
            test_coverage: None,
            has_tests: false,
            has_documentation: true,
            embedding: None,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        };

        cognitive_manager
            .remember_unit(&unit)
            .await
            .expect("Failed to store unit");
    }

    // Step 7: Verify storage
    let stats = cognitive_manager
        .get_statistics()
        .await
        .expect("Failed to get statistics");

    assert!(stats.semantic.total_units >= processed.chunks.len() as u64);
}

#[tokio::test]
async fn test_e2e_multi_agent_session_fork_modify_merge_verify() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_multi_agent_test".to_string(),
        database: "multi_agent".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    // Agent 1: Create initial workspace
    let vfs1 = VirtualFileSystem::new(connection_manager.clone());
    let cognitive1 = CognitiveManager::new(connection_manager.clone());

    let workspace_id = uuid::Uuid::new_v4();
    let project_id = CortexId::new();

    let base_path = VirtualPath::new("main.rs").expect("Invalid path");
    vfs1.write_file(&workspace_id, &base_path, b"fn main() {}")
        .await
        .expect("Failed to write file");

    let mut episode1 = EpisodicMemory::new(
        "Agent 1: Created main.rs".to_string(),
        "agent-1".to_string(),
        project_id,
        EpisodeType::Feature,
    );
    episode1.entities_created = vec!["main.rs".to_string()];
    cognitive1
        .remember_episode(&episode1)
        .await
        .expect("Failed to store episode");

    // Agent 2: Fork and modify
    let vfs2 = VirtualFileSystem::new(connection_manager.clone());
    let cognitive2 = CognitiveManager::new(connection_manager.clone());

    // In a real scenario, this would be a fork operation
    // For this test, we simulate by modifying the same workspace
    let modified_path = VirtualPath::new("main.rs").expect("Invalid path");
    vfs2.write_file(
        &workspace_id,
        &modified_path,
        b"fn main() { println!(\"Hello\"); }",
    )
    .await
    .expect("Failed to modify file");

    let mut episode2 = EpisodicMemory::new(
        "Agent 2: Added println".to_string(),
        "agent-2".to_string(),
        project_id,
        EpisodeType::Feature,
    );
    episode2.entities_modified = vec!["main.rs".to_string()];
    cognitive2
        .remember_episode(&episode2)
        .await
        .expect("Failed to store episode");

    // Verify both episodes exist
    let stats = cognitive1
        .get_statistics()
        .await
        .expect("Failed to get statistics");

    assert_eq!(stats.episodic.total_episodes, 2);

    // Verify final file state
    let content = vfs1
        .read_file(&workspace_id, &modified_path)
        .await
        .expect("Failed to read file");

    assert!(String::from_utf8_lossy(&content).contains("println"));
}

// =============================================================================
// Test 7: Performance and Stress Tests
// =============================================================================

#[tokio::test]
async fn test_performance_high_volume_episodes() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig::default(),
        namespace: "cortex_perf_test".to_string(),
        database: "perf".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let cognitive_manager = CognitiveManager::new(connection_manager);
    let project_id = CortexId::new();

    let start = std::time::Instant::now();

    // Create 100 episodes
    for i in 0..100 {
        let episode = EpisodicMemory::new(
            format!("Task {}", i),
            "perf-agent".to_string(),
            project_id,
            EpisodeType::Task,
        );

        cognitive_manager
            .remember_episode(&episode)
            .await
            .expect("Failed to store episode");
    }

    let duration = start.elapsed();

    // Should complete in reasonable time (< 5 seconds for 100 episodes)
    assert!(duration.as_secs() < 5);

    // Verify all episodes stored
    let stats = cognitive_manager
        .get_statistics()
        .await
        .expect("Failed to get statistics");

    assert_eq!(stats.episodic.total_episodes, 100);
}

#[tokio::test]
async fn test_concurrent_vfs_operations() {
    let db_config = DatabaseConfig {
        connection_mode: ConnectionMode::Local {
            endpoint: "mem://".to_string(),
        },
        credentials: Credentials::default(),
        pool_config: cortex_storage::prelude::PoolConfig {
            min_connections: 5,
            max_connections: 10,
            ..Default::default()
        },
        namespace: "cortex_concurrent_test".to_string(),
        database: "concurrent".to_string(),
    };

    let connection_manager = Arc::new(
        ConnectionManager::new(db_config)
            .await
            .expect("Failed to create connection manager"),
    );

    let vfs = Arc::new(VirtualFileSystem::new(connection_manager));
    let workspace_id = uuid::Uuid::new_v4();

    // Spawn multiple concurrent write operations
    let mut handles = vec![];

    for i in 0..10 {
        let vfs_clone = vfs.clone();
        let ws_id = workspace_id;

        let handle = tokio::spawn(async move {
            let path = VirtualPath::new(&format!("file_{}.txt", i)).expect("Invalid path");
            vfs_clone
                .write_file(&ws_id, &path, format!("Content {}", i).as_bytes())
                .await
                .expect("Failed to write file");
        });

        handles.push(handle);
    }

    // Wait for all operations
    for handle in handles {
        handle.await.expect("Task panicked");
    }

    // Verify all files exist
    for i in 0..10 {
        let path = VirtualPath::new(&format!("file_{}.txt", i)).expect("Invalid path");
        let exists = vfs.exists(&workspace_id, &path).await.unwrap_or(false);
        assert!(exists, "File {} should exist", i);
    }
}
